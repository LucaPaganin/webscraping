{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb766dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded:\n",
      "\n",
      "DATA:\n",
      "  - data_dir: /mnt/c/Users/LP031/onedrive_unige/data/immobiliare.it\n",
      "\n",
      "FILTERING:\n",
      "  - min_occurrences:\n",
      "    - default: 20\n",
      "    - city: 40\n",
      "    - zone: 15\n",
      "    - typology_name: 10\n",
      "    - garage_type: 25\n",
      "    - garage_primary_type: 25\n",
      "  - feature_threshold: 100\n",
      "  - typologies_to_keep: ['Appartamento', 'Attico']\n",
      "\n",
      "ML:\n",
      "  - test_size: 0.2\n",
      "  - random_state: 42\n",
      "  - n_estimators: 100\n",
      "  - top_features_to_select: 20\n",
      "\n",
      "FLOOR_MAPPING:\n",
      "  - piano terra: 0\n",
      "  - seminterrato: 0\n",
      "  - interrato (-1): -1\n",
      "  - interrato (-2): -2\n",
      "  - interrato (-3): -3\n",
      "  - piano rialzato: 0.5\n",
      "  - ammezzato: 0.5\n",
      "  - su più livelli: None\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "CONFIG = {\n",
    "    # Data paths\n",
    "    \"data\": {\n",
    "        \"data_dir\": \"/mnt/c/Users/LP031/onedrive_unige/data/immobiliare.it\",\n",
    "    },\n",
    "    \n",
    "    # Filtering thresholds\n",
    "    \"filtering\": {\n",
    "        # Column-specific minimum occurrences thresholds\n",
    "        \"min_occurrences\": {\n",
    "            \"default\": 20,  # Default threshold for categorical values\n",
    "            \"city\": 40,     # City-specific threshold\n",
    "            \"zone\": 15,     # Zone-specific threshold\n",
    "            \"typology_name\": 10,  # Property type threshold\n",
    "            \"garage_type\": 25,    # Garage type threshold\n",
    "            \"garage_primary_type\": 25  # Garage primary type threshold\n",
    "        },\n",
    "        \"feature_threshold\": 100,  # Minimum occurrences for binary feature extraction from ga4features\n",
    "        \"typologies_to_keep\": [\"Appartamento\", \"Attico\"]  # Property types to keep in the analysis\n",
    "    },\n",
    "    \n",
    "    # Machine learning parameters\n",
    "    \"ml\": {\n",
    "        \"test_size\": 0.2,  # Percentage of data to use for testing\n",
    "        \"random_state\": 42,  # Random seed for reproducibility\n",
    "        \"n_estimators\": 100,  # Number of estimators for tree-based models\n",
    "        \"top_features_to_select\": 20  # Number of top features to select in feature selection methods\n",
    "    },\n",
    "    \n",
    "    # Floor normalization mapping\n",
    "    \"floor_mapping\": {\n",
    "        \"piano terra\": 0,\n",
    "        \"seminterrato\": 0,\n",
    "        \"interrato (-1)\": -1,\n",
    "        \"interrato (-2)\": -2,\n",
    "        \"interrato (-3)\": -3,\n",
    "        \"piano rialzato\": 0.5,\n",
    "        \"ammezzato\": 0.5,\n",
    "        \"su più livelli\": None  # Will be converted to np.nan\n",
    "    }\n",
    "}\n",
    "\n",
    "# Print configuration for reference\n",
    "print(\"Configuration loaded:\")\n",
    "for section, params in CONFIG.items():\n",
    "    print(f\"\\n{section.upper()}:\")\n",
    "    for key, value in params.items():\n",
    "        if isinstance(value, dict):\n",
    "            print(f\"  - {key}:\")\n",
    "            for subkey, subvalue in value.items():\n",
    "                print(f\"    - {subkey}: {subvalue}\")\n",
    "        else:\n",
    "            print(f\"  - {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581a303a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://consulting_feed_token:****@pkgs.dev.azure.com/RinaCUBE/a11c8c85-c3a2-4e18-ab34-382599832cf2/_packaging/Consulting_Libs/pypi/simple/\n",
      "Requirement already satisfied: xgboost in /home/lucapaganin/mystuff/webscraping/.venv/lib/python3.10/site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in /home/lucapaganin/mystuff/webscraping/.venv/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /home/lucapaganin/mystuff/webscraping/.venv/lib/python3.10/site-packages (from xgboost) (2.27.5)\n",
      "Requirement already satisfied: scipy in /home/lucapaganin/mystuff/webscraping/.venv/lib/python3.10/site-packages (from xgboost) (1.15.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Missing values per column:\n",
      "matchSearch        1347\n",
      "propertiesCount    1347\n",
      "price_max          1347\n",
      "price_min          1347\n",
      "description        1345\n",
      "ga4Garage           976\n",
      "views               767\n",
      "elevator            657\n",
      "price_value         517\n",
      "caption             387\n",
      "macrozone           297\n",
      "photo_caption       280\n",
      "agency_name         214\n",
      "agency_id           214\n",
      "agency_type         214\n",
      "dtype: int64\n",
      "\n",
      "Selected columns that exist in the dataset:\n",
      "['price_value', 'surface', 'rooms', 'bathrooms', 'floor', 'typology_name', 'elevator', 'ga4Heating', 'ga4Garage', 'ga4features', 'latitude', 'longitude', 'city', 'macrozone', 'isNew', 'luxury']\n",
      "\n",
      "Selected columns missing from the dataset:\n",
      "['zone']\n",
      "\n",
      "Using 'macrozone' instead of 'zone' for location information\n",
      "\n",
      "Subset DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "typology_name\n",
       "Appartamento               1274\n",
       "Attico                       26\n",
       "Villa unifamiliare           22\n",
       "Appartamento in villa         6\n",
       "Palazzo - Edificio            5\n",
       "Villa bifamiliare             3\n",
       "Mansarda                      3\n",
       "Loft                          2\n",
       "Open space                    2\n",
       "Villa a schiera               1\n",
       "Sasso                         1\n",
       "Villa plurifamiliare          1\n",
       "Terratetto unifamiliare       1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pip install xgboost\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import re\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Data preprocessing and modeling\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer, TransformedTargetRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, RFE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, FunctionTransformer\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Use data directory from configuration\n",
    "data_dir = CONFIG[\"data\"][\"data_dir\"]\n",
    "# Load data from CSV\n",
    "df = pd.concat([\n",
    "    pd.read_csv(f'{data_dir}/ads_savona_rent.csv'),\n",
    "    pd.read_csv(f'{data_dir}/ads_genova_rent.csv')\n",
    "], ignore_index=True)\n",
    "\n",
    "# Let's select a subset of relevant features for our ML model\n",
    "# We'll focus on physical property attributes, location data, and price\n",
    "\n",
    "# First, examine missing values to help with feature selection\n",
    "print(\"Missing values per column:\")\n",
    "print(df.isnull().sum().sort_values(ascending=False).head(15))\n",
    "\n",
    "# Create a list of features we think are important for predicting property prices\n",
    "relevant_columns = [\n",
    "    # Target variable\n",
    "    'price_value',\n",
    "    \n",
    "    # Core property attributes\n",
    "    'surface',          # Size of the property\n",
    "    'rooms',            # Number of rooms\n",
    "    'bathrooms',        # Number of bathrooms\n",
    "    'floor',            # Floor level\n",
    "    'typology_name',    # Type of property (apartment, house, etc.)\n",
    "    \n",
    "    # Amenities/features\n",
    "    'elevator',         # Presence of elevator\n",
    "    'ga4Heating',       # Heating type\n",
    "    'ga4Garage',        # Garage availability\n",
    "    'ga4features',      # Additional features (e.g., garden, pool, etc.)\n",
    "    \n",
    "    # Location data (important for real estate)\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'city',\n",
    "    'macrozone',        # Area within the city\n",
    "    'zone',             # More specific location\n",
    "    \n",
    "    # Additional attributes that might influence price\n",
    "    'isNew',            # New construction or not\n",
    "    'luxury'            # Luxury property flag\n",
    "]\n",
    "\n",
    "# Check which columns from our selection actually exist in the dataset\n",
    "existing_columns = [col for col in relevant_columns if col in df.columns]\n",
    "missing_columns = [col for col in relevant_columns if col not in df.columns]\n",
    "\n",
    "print(\"\\nSelected columns that exist in the dataset:\")\n",
    "print(existing_columns)\n",
    "\n",
    "print(\"\\nSelected columns missing from the dataset:\")\n",
    "print(missing_columns)\n",
    "\n",
    "# Create our dataset with only the relevant columns that exist\n",
    "if 'zone' not in df.columns and 'macrozone' in df.columns:\n",
    "    # If zone doesn't exist but macrozone does, use macrozone\n",
    "    print(\"\\nUsing 'macrozone' instead of 'zone' for location information\")\n",
    "    df['zone'] = df['macrozone']\n",
    "\n",
    "# Create the subset dataframe\n",
    "df_subset = df[existing_columns].copy()\n",
    "\n",
    "# Display the first few rows of our subset\n",
    "print(\"\\nSubset DataFrame:\")\n",
    "df_subset.head()\n",
    "df_subset['typology_name'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293b62ac",
   "metadata": {},
   "source": [
    "# Filter the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b02060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to include only rows with specific typology names from config\n",
    "if 'typology_name' in df_subset.columns:\n",
    "    df_subset = df_subset[df_subset['typology_name'].isin(CONFIG['filtering']['typologies_to_keep'])].copy()\n",
    "    print(f\"Filtered df_subset shape: {df_subset.shape}\")\n",
    "else:\n",
    "    print(\"'typology_name' column not found in df_subset.\")\n",
    "    \n",
    "# Filter out all cities having less than min_occurrences from config\n",
    "# Get the city-specific threshold\n",
    "min_occurrences_city = CONFIG['filtering']['min_occurrences']['city']\n",
    "city_counts = df_subset['city'].value_counts()\n",
    "cities_to_keep = city_counts[city_counts >= min_occurrences_city].index\n",
    "df_subset = df_subset[df_subset['city'].isin(cities_to_keep)]\n",
    "print(f\"Filtered cities with fewer than {min_occurrences_city} occurrences.\")\n",
    "print(f\"Remaining cities: {len(cities_to_keep)} out of {len(city_counts)}\")\n",
    "df_subset.city.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4bfb84",
   "metadata": {},
   "source": [
    "# Floor Number Normalization\n",
    "\n",
    "The `floor_number` column contains a mix of formats, including:\n",
    "- Simple numeric values ('1', '2', '3', etc.)\n",
    "- Italian text descriptions ('piano terra', 'piano rialzato')\n",
    "- Floor ranges ('da 2 a 3', 'da seminterrato a piano terra')\n",
    "- Multi-level properties ('su più livelli', 'piano terra, 1')\n",
    "\n",
    "We'll normalize these values by converting them into numeric values, where:\n",
    "- -3, -2, -1: Underground floors (interrato)\n",
    "- 0: Ground floor (piano terra, seminterrato)\n",
    "- 0.5: Mezzanine/raised ground floor (piano rialzato, ammezzato)\n",
    "- 1-25: Standard floors\n",
    "- For ranges or multiple floors, we'll use the highest floor as it typically represents the property's main position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda5f4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to normalize floor numbers\n",
    "\n",
    "def normalize_floor(floor_str):\n",
    "    \"\"\"\n",
    "    Normalize floor number strings to numeric values.\n",
    "    \n",
    "    Args:\n",
    "        floor_str: String representation of floor(s)\n",
    "        \n",
    "    Returns:\n",
    "        float: Normalized floor number\n",
    "        \n",
    "    Examples:\n",
    "        '3' -> 3.0\n",
    "        'piano terra' -> 0.0\n",
    "        'da 2 a 5' -> 5.0 (taking the highest value)\n",
    "        'seminterrato, 2' -> 2.0 (taking the highest value)\n",
    "    \"\"\"\n",
    "    if pd.isna(floor_str):\n",
    "        return np.nan\n",
    "    \n",
    "    floor_str = str(floor_str).lower().strip()\n",
    "    \n",
    "    # Use floor mapping from configuration\n",
    "    floor_mapping = CONFIG['floor_mapping']\n",
    "    # Convert None to np.nan\n",
    "    floor_mapping = {k: (np.nan if v is None else v) for k, v in floor_mapping.items()}\n",
    "    \n",
    "    # Check direct mapping first\n",
    "    if floor_str in floor_mapping:\n",
    "        return float(floor_mapping[floor_str])\n",
    "    \n",
    "    # For complex strings, extract all numbers\n",
    "    numbers = []\n",
    "    \n",
    "    # Look for Italian range pattern \"da X a Y\"\n",
    "    range_match = re.search(r'da\\s+(\\d+|\\w+)\\s+a\\s+(\\d+|\\w+)', floor_str)\n",
    "    if range_match:\n",
    "        # Extract the second number (end of range)\n",
    "        end_value = range_match.group(2)\n",
    "        if end_value.isdigit():\n",
    "            numbers.append(int(end_value))\n",
    "        # If the end value is a word (like \"piano terra\"), map it\n",
    "        elif end_value in floor_mapping:\n",
    "            numbers.append(floor_mapping[end_value])\n",
    "    \n",
    "    # Extract all digits\n",
    "    digit_matches = re.findall(r'\\d+', floor_str)\n",
    "    numbers.extend([int(d) for d in digit_matches])\n",
    "    \n",
    "    # Extract all known floor types\n",
    "    for term, value in floor_mapping.items():\n",
    "        if term in floor_str and not pd.isna(value):  # Skip None/NaN values\n",
    "            numbers.append(value)\n",
    "    \n",
    "    # Return the highest floor (most relevant for pricing)\n",
    "    if numbers:\n",
    "        return float(max(numbers))\n",
    "    \n",
    "    # If no numbers found, return NaN\n",
    "    return np.nan\n",
    "\n",
    "# Test the function on our unique values\n",
    "floor_test_df = pd.DataFrame({'floor_number': df['floor_number'].unique()})\n",
    "floor_test_df['normalized_floor'] = floor_test_df['floor_number'].apply(normalize_floor)\n",
    "\n",
    "# Display the results to verify the normalization\n",
    "floor_test_df.sort_values('normalized_floor').head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48f3307",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform more comprehensive data preprocessing on our subset\n",
    "\n",
    "# Handle missing values and data conversion\n",
    "df_clean = df_subset.copy()\n",
    "\n",
    "# Convert string columns to appropriate numeric types\n",
    "for col in ['surface', 'rooms', 'bathrooms']:\n",
    "    if col in df_clean.columns:\n",
    "        # Extract numbers from strings if necessary and convert to float\n",
    "        if df_clean[col].dtype == 'object':\n",
    "            df_clean[col] = df_clean[col].str.extract('(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "# Handle floor information using our normalize_floor function\n",
    "if 'floor_number' in df_clean.columns:\n",
    "    # Apply our normalized floor function\n",
    "    df_clean['floor_numeric'] = df_clean['floor_number'].apply(normalize_floor)\n",
    "    \n",
    "    # Drop the original floor column and keep the numeric version\n",
    "    df_clean.drop('floor_number', axis=1, inplace=True)\n",
    "\n",
    "elif 'floor' in df_clean.columns:\n",
    "    # Apply our normalized floor function\n",
    "    df_clean['floor_numeric'] = df_clean['floor'].apply(normalize_floor)\n",
    "    \n",
    "    # Drop the original floor column and keep the numeric version\n",
    "    df_clean.drop('floor', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8eef93b",
   "metadata": {},
   "source": [
    "## Clean up and collapse garage options\n",
    "The `garage` column contains various formats and descriptions. We'll standardize these to a few categories\n",
    "- 'no park': No parking space\n",
    "- 'common park': common parking space\n",
    "- 'private park': Private parking space\n",
    "- 'common box': Common garage\n",
    "- 'private box': Private garage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c83b673",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['ga4Garage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136df826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to standardize garage options\n",
    "def standardize_garage(garage_str):\n",
    "    \"\"\"\n",
    "    Standardize garage descriptions into five categories:\n",
    "    - 'no park': No parking space\n",
    "    - 'common park': Common parking space\n",
    "    - 'private park': Private parking space (not used in this case)\n",
    "    - 'common box': Common garage/box\n",
    "    - 'private box': Private garage/box\n",
    "    \n",
    "    For combined options, prioritize private box > common box > common park > no park\n",
    "    \n",
    "    Args:\n",
    "        garage_str: String description of garage options\n",
    "        \n",
    "    Returns:\n",
    "        str: Standardized category\n",
    "    \"\"\"\n",
    "    if pd.isna(garage_str):\n",
    "        return 'no park'\n",
    "    \n",
    "    garage_str = str(garage_str).lower()\n",
    "    \n",
    "    # Check for box privato (private box/garage)\n",
    "    if 'box privato' in garage_str:\n",
    "        return 'private box'\n",
    "    \n",
    "    # Check for parcheggio/garage comune (common parking/garage)\n",
    "    if 'parcheggio' in garage_str or 'garage comune' in garage_str:\n",
    "        return 'common park'\n",
    "    \n",
    "    # Default to no parking if nothing matches\n",
    "    return 'no park'\n",
    "\n",
    "# Let's create a more detailed function that considers counts and combinations\n",
    "def detailed_garage_standardization(garage_str):\n",
    "    \"\"\"\n",
    "    Create a more detailed standardization of garage options that considers:\n",
    "    1. The priority type (private box > common park)\n",
    "    2. The count of parking spaces\n",
    "    \n",
    "    Args:\n",
    "        garage_str: String description of garage options\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with standardized categorization\n",
    "              - 'primary_type': Main parking type (private_box, common_park, no_park)\n",
    "              - 'private_box_count': Number of private boxes\n",
    "              - 'common_park_count': Number of common parking spaces\n",
    "              - 'total_count': Total number of parking spaces\n",
    "    \"\"\"\n",
    "    if pd.isna(garage_str):\n",
    "        return {\n",
    "            'primary_type': 'no_park',\n",
    "            'private_box_count': 0,\n",
    "            'common_park_count': 0,\n",
    "            'total_count': 0\n",
    "        }\n",
    "    \n",
    "    garage_str = str(garage_str).lower()\n",
    "    \n",
    "    # Initialize counts\n",
    "    private_box_count = 0\n",
    "    common_park_count = 0\n",
    "    \n",
    "    # Look for private box counts\n",
    "    if 'box privato' in garage_str:\n",
    "        # Extract the count before \"in box privato\"\n",
    "        box_matches = re.findall(r'(\\d+)\\s+in\\s+box\\s+privato', garage_str)\n",
    "        if box_matches:\n",
    "            private_box_count = sum(int(count) for count in box_matches)\n",
    "    \n",
    "    # Look for common parking counts\n",
    "    if 'parcheggio' in garage_str or 'garage comune' in garage_str:\n",
    "        # Extract the count before \"in parcheggio/garage comune\"\n",
    "        park_matches = re.findall(r'(\\d+)\\s+in\\s+parcheggio', garage_str)\n",
    "        if park_matches:\n",
    "            common_park_count = sum(int(count) for count in park_matches)\n",
    "    \n",
    "    # Determine primary type based on priority\n",
    "    primary_type = 'no_park'\n",
    "    if private_box_count > 0:\n",
    "        primary_type = 'private_box'\n",
    "    elif common_park_count > 0:\n",
    "        primary_type = 'common_park'\n",
    "    \n",
    "    return {\n",
    "        'primary_type': primary_type,\n",
    "        'private_box_count': private_box_count,\n",
    "        'common_park_count': common_park_count,\n",
    "        'total_count': private_box_count + common_park_count\n",
    "    }\n",
    "\n",
    "# Apply the standardization to the DataFrame\n",
    "if 'ga4Garage' in df_clean.columns:\n",
    "    # Add a simple categorization\n",
    "    df_clean['garage_type'] = df_clean['ga4Garage'].apply(standardize_garage)\n",
    "    \n",
    "    # Add detailed garage information\n",
    "    garage_details = df_clean['ga4Garage'].apply(detailed_garage_standardization)\n",
    "    df_clean['garage_primary_type'] = garage_details.apply(lambda x: x['primary_type'])\n",
    "    df_clean['garage_total_count'] = garage_details.apply(lambda x: x['total_count'])\n",
    "    \n",
    "    # Drop the original column as we now have standardized versions\n",
    "    df_clean.drop('ga4Garage', axis=1, inplace=True)\n",
    "\n",
    "# Show the distribution of standardized garage types\n",
    "if 'garage_type' in df_clean.columns:\n",
    "    print(\"Standardized garage types distribution:\")\n",
    "    print(df_clean['garage_type'].value_counts())\n",
    "    \n",
    "    print(\"\\nDetailed garage primary types:\")\n",
    "    print(df_clean['garage_primary_type'].value_counts())\n",
    "    \n",
    "    print(\"\\nGarage space count distribution:\")\n",
    "    print(df_clean['garage_total_count'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f03c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_counts = df_clean['ga4features'].str.split(',').explode().str.strip().value_counts()\n",
    "print(\"Individual features and their counts:\")\n",
    "print(feature_counts)\n",
    "\n",
    "# Check how many values meet our threshold from config\n",
    "threshold = CONFIG['filtering']['feature_threshold']\n",
    "top_features = feature_counts[feature_counts >= threshold].index.tolist()\n",
    "print(f\"\\nFeatures with {threshold} or more occurrences: {len(top_features)}\")\n",
    "print(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a0584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary columns for features that occur at least feature_threshold times\n",
    "threshold = CONFIG['filtering']['feature_threshold']\n",
    "top_features = feature_counts[feature_counts >= threshold].index.tolist()\n",
    "\n",
    "# Function to check if a feature is present in the comma-separated list\n",
    "def has_feature(features_str, feature):\n",
    "    if pd.isna(features_str):\n",
    "        return 0\n",
    "    \n",
    "    feature_list = [f.strip() for f in str(features_str).split(',')]\n",
    "    return 1 if feature in feature_list else 0\n",
    "\n",
    "# Create binary columns for each top feature\n",
    "for feature in top_features:\n",
    "    # Create a sanitized column name (replace spaces with underscores, lowercase)\n",
    "    feature_col_name = f\"has_{feature.replace(' ', '_').lower()}\"\n",
    "    \n",
    "    # Apply the function to create binary indicators\n",
    "    df_clean[feature_col_name] = df_clean['ga4features'].apply(\n",
    "        lambda x: has_feature(x, feature)\n",
    "    )\n",
    "\n",
    "# Drop the original ga4features column as we now have individual feature columns\n",
    "df_clean.drop('ga4features', axis=1, inplace=True)\n",
    "\n",
    "# Display the new binary columns\n",
    "print(\"Sample of the DataFrame with new binary feature columns:\")\n",
    "print(df_clean[['price_value'] + [f\"has_{feature.replace(' ', '_').lower()}\" for feature in top_features[:5]]].head())\n",
    "\n",
    "# Show the total counts for each binary feature to verify\n",
    "binary_counts = pd.DataFrame({\n",
    "    'feature': top_features,\n",
    "    'count': [df_clean[f\"has_{feature.replace(' ', '_').lower()}\"].sum() for feature in top_features]\n",
    "}).sort_values('count', ascending=False)\n",
    "\n",
    "print(\"\\nBinary feature counts:\")\n",
    "print(binary_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ddc782",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e4a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out features that have less than min_occurrences occurrences\n",
    "# Use column-specific thresholds from config\n",
    "min_occurrences_dict = CONFIG['filtering']['min_occurrences']\n",
    "default_min_occurrences = min_occurrences_dict['default']\n",
    "\n",
    "# List of categorical columns to check for filtering\n",
    "categorical_cols_to_filter = ['city', 'zone', 'typology_name', 'garage_type', 'garage_primary_type']\n",
    "categorical_cols_to_filter = [col for col in categorical_cols_to_filter if col in df_clean.columns]\n",
    "\n",
    "print(f\"Filtering out values with fewer than specified occurrences in categorical columns:\")\n",
    "original_shape = df_clean.shape[0]\n",
    "\n",
    "# For each categorical column, filter out values with fewer than min_occurrences\n",
    "for col in categorical_cols_to_filter:\n",
    "    # Get column-specific threshold (or default if not specified)\n",
    "    threshold = min_occurrences_dict.get(col, default_min_occurrences)\n",
    "    \n",
    "    # Get value counts\n",
    "    value_counts = df_clean[col].value_counts()\n",
    "    \n",
    "    # Find values to keep (those with at least the threshold occurrences)\n",
    "    values_to_keep = value_counts[value_counts >= threshold].index\n",
    "    \n",
    "    # Filter the dataframe\n",
    "    df_clean = df_clean[df_clean[col].isin(values_to_keep)]\n",
    "    \n",
    "    # Print how many values were kept for this column\n",
    "    print(f\"  - {col}: Kept {len(values_to_keep)} unique values out of {len(value_counts)} using threshold {threshold}\")\n",
    "    print(f\"    Remaining records: {df_clean.shape[0]}\")\n",
    "\n",
    "# Show how many records were filtered out\n",
    "records_removed = original_shape - df_clean.shape[0]\n",
    "print(f\"\\nFiltering removed {records_removed} records ({records_removed/original_shape:.1%} of the original dataset)\")\n",
    "print(f\"Shape after filtering: {df_clean.shape}\")\n",
    "\n",
    "# Show distribution of values in the filtered columns\n",
    "for col in categorical_cols_to_filter:\n",
    "    print(f\"\\n{col} value distribution after filtering:\")\n",
    "    print(df_clean[col].value_counts().head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225289d2",
   "metadata": {},
   "source": [
    "## Handle categorical columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67295f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle categorical features - exclude binary feature columns which are already 0/1\n",
    "categorical_cols = ['typology_name', 'city', 'zone', 'elevator', 'ga4Heating', 'garage_type', 'garage_primary_type']\n",
    "categorical_cols = [col for col in categorical_cols if col in df_clean.columns]\n",
    "\n",
    "# Separate binary feature columns (starting with 'has_')\n",
    "binary_feature_cols = [col for col in df_clean.columns if col.startswith('has_')]\n",
    "\n",
    "# Handle numeric features\n",
    "numeric_cols = ['surface', 'rooms', 'bathrooms', 'garage_total_count', 'floor_numeric']\n",
    "numeric_cols = [col for col in numeric_cols if col in df_clean.columns]\n",
    "\n",
    "# Fill missing values in numeric columns\n",
    "for col in numeric_cols:\n",
    "    if col in df_clean.columns:\n",
    "        df_clean[col] = df_clean[col].fillna(df_clean[col].median())\n",
    "\n",
    "# Encode categorical variables\n",
    "for col in categorical_cols:\n",
    "    if df_clean[col].dtype == 'object':\n",
    "        # Fill missing values with a placeholder\n",
    "        df_clean[col] = df_clean[col].fillna('Unknown')\n",
    "        \n",
    "        # Create dummies with drop_first=True to avoid multicollinearity\n",
    "        dummies = pd.get_dummies(df_clean[col], prefix=col, drop_first=True)\n",
    "        \n",
    "        # Add dummies to dataframe and drop the original column\n",
    "        df_clean = pd.concat([df_clean, dummies], axis=1)\n",
    "        df_clean.drop(col, axis=1, inplace=True)\n",
    "\n",
    "# Handle boolean features\n",
    "bool_cols = ['isNew', 'luxury'] + binary_feature_cols  # Include our binary feature columns\n",
    "bool_cols = [col for col in bool_cols if col in df_clean.columns]\n",
    "\n",
    "for col in bool_cols:\n",
    "    df_clean[col] = df_clean[col].fillna(0).astype(int)\n",
    "\n",
    "# Drop rows with missing target values\n",
    "df_clean = df_clean.dropna(subset=['price_value'])\n",
    "\n",
    "# Display the processed dataset\n",
    "print(\"Processed dataset shape:\", df_clean.shape)\n",
    "print(\"\\nProcessed dataset columns:\")\n",
    "print(df_clean.columns.tolist())\n",
    "print(\"\\nMissing values in processed dataset:\")\n",
    "print(df_clean.isnull().sum().sum())\n",
    "\n",
    "# Preview the processed dataset\n",
    "print(\"\\nPreview of the processed dataset (first 3 rows, selected columns):\")\n",
    "preview_cols = ['price_value']\n",
    "if 'floor_numeric' in df_clean.columns:\n",
    "    preview_cols.append('floor_numeric')\n",
    "if 'garage_total_count' in df_clean.columns:\n",
    "    preview_cols.append('garage_total_count')\n",
    "if binary_feature_cols:\n",
    "    preview_cols += binary_feature_cols[:3]  # Add up to 3 binary feature columns\n",
    "print(df_clean[preview_cols].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdec936",
   "metadata": {},
   "source": [
    "# Reusable Preprocessing Pipeline\n",
    "\n",
    "Let's create a scikit-learn pipeline to make our preprocessing reusable and consistent. This will:\n",
    "1. Handle all preprocessing steps in a single object\n",
    "2. Apply the exact same transformations to new data during inference\n",
    "3. Manage categorical variables properly, even when new categories appear\n",
    "4. Preserve feature names for interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34db2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer for extracting numbers from strings\n",
    "class NumericExtractor(BaseEstimator, TransformerMixin):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "        \n",
    "    def transform(self, X):\n",
    "        return X.apply(lambda x: pd.to_numeric(\n",
    "            re.search(r'(\\d+\\.?\\d*)', str(x)).group(1) if isinstance(x, str) and re.search(r'(\\d+\\.?\\d*)', str(x)) else x, \n",
    "            errors='coerce'\n",
    "        ))\n",
    "\n",
    "# Custom transformer for floor normalization\n",
    "class FloorNormalizer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        # Use floor mapping from configuration\n",
    "        self.floor_mapping = CONFIG['floor_mapping']\n",
    "        # Convert None to np.nan\n",
    "        self.floor_mapping = {k: (np.nan if v is None else v) for k, v in self.floor_mapping.items()}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X.apply(self._normalize_floor)\n",
    "    \n",
    "    def _normalize_floor(self, floor_str):\n",
    "        # Check if this is a Series (when applied to a DataFrame column)\n",
    "        if isinstance(floor_str, pd.Series):\n",
    "            # Apply the function to each element in the Series\n",
    "            return floor_str.apply(self._normalize_floor)\n",
    "            \n",
    "        # Process a single value\n",
    "        if pd.isna(floor_str):\n",
    "            return np.nan\n",
    "        \n",
    "        floor_str = str(floor_str).lower().strip()\n",
    "        \n",
    "        # Check direct mapping first\n",
    "        if floor_str in self.floor_mapping:\n",
    "            return float(self.floor_mapping[floor_str])\n",
    "        \n",
    "        # For complex strings, extract all numbers\n",
    "        numbers = []\n",
    "        \n",
    "        # Look for Italian range pattern \"da X a Y\"\n",
    "        range_match = re.search(r'da\\s+(\\d+|\\w+)\\s+a\\s+(\\d+|\\w+)', floor_str)\n",
    "        if range_match:\n",
    "            # Extract the second number (end of range)\n",
    "            end_value = range_match.group(2)\n",
    "            if end_value.isdigit():\n",
    "                numbers.append(int(end_value))\n",
    "            # If the end value is a word (like \"piano terra\"), map it\n",
    "            elif end_value in self.floor_mapping:\n",
    "                numbers.append(self.floor_mapping[end_value])\n",
    "        \n",
    "        # Extract all digits\n",
    "        digit_matches = re.findall(r'\\d+', floor_str)\n",
    "        numbers.extend([int(d) for d in digit_matches])\n",
    "        \n",
    "        # Extract all known floor types\n",
    "        for term, value in self.floor_mapping.items():\n",
    "            if term in floor_str and not pd.isna(value):  # Skip None/NaN values\n",
    "                numbers.append(value)\n",
    "        \n",
    "        # Return the highest floor (most relevant for pricing)\n",
    "        if numbers:\n",
    "            return float(max(numbers))\n",
    "        \n",
    "        # If no numbers found, return NaN\n",
    "        return np.nan\n",
    "\n",
    "# Define a function that creates and returns a complete preprocessing pipeline\n",
    "def create_preprocessing_pipeline(df=None):\n",
    "    \"\"\"\n",
    "    Creates a scikit-learn preprocessing pipeline for the real estate data.\n",
    "    \n",
    "    Args:\n",
    "        df: Optional DataFrame used for detecting columns.\n",
    "              If None, the pipeline must be initialized with explicit columns later.\n",
    "    \n",
    "    Returns:\n",
    "        A tuple containing:\n",
    "        - preprocessor: A scikit-learn ColumnTransformer\n",
    "        - column_info: A dictionary with information about detected columns\n",
    "    \"\"\"\n",
    "    # Initialize column info dictionary\n",
    "    column_info = {\n",
    "        'numeric_cols': [],\n",
    "        'floor_col': None,\n",
    "        'categorical_cols': [],\n",
    "        'bool_cols': [],\n",
    "        'binary_features_cols': []\n",
    "    }\n",
    "    \n",
    "    if df is not None:\n",
    "        # Auto-detect columns from the dataframe\n",
    "        numeric_cols = ['surface', 'rooms', 'bathrooms', 'garage_total_count'] \n",
    "        numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "        column_info['numeric_cols'] = numeric_cols\n",
    "        \n",
    "        floor_col = 'floor_number' if 'floor_number' in df.columns else 'floor'\n",
    "        floor_col = floor_col if floor_col in df.columns else None\n",
    "        column_info['floor_col'] = floor_col\n",
    "        \n",
    "        categorical_cols = ['typology_name', 'city', 'zone', 'elevator', 'ga4Heating', \n",
    "                           'garage_type', 'garage_primary_type']\n",
    "        categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "        column_info['categorical_cols'] = categorical_cols\n",
    "        \n",
    "        bool_cols = ['isNew', 'luxury']\n",
    "        bool_cols = [col for col in bool_cols if col in df.columns]\n",
    "        column_info['bool_cols'] = bool_cols\n",
    "        \n",
    "        # Detect binary feature columns (starting with 'has_')\n",
    "        binary_features_cols = [col for col in df.columns if col.startswith('has_')]\n",
    "        column_info['binary_features_cols'] = binary_features_cols\n",
    "    \n",
    "    # Create transformers for each column type\n",
    "    transformers = []\n",
    "    \n",
    "    # Numeric columns: extract numbers, impute missing, and scale\n",
    "    if column_info['numeric_cols']:\n",
    "        numeric_pipeline = Pipeline([\n",
    "            ('extract_numeric', NumericExtractor()),\n",
    "            ('impute', SimpleImputer(strategy='median')),\n",
    "            ('scale', StandardScaler())\n",
    "        ])\n",
    "        transformers.append(('numeric', numeric_pipeline, column_info['numeric_cols']))\n",
    "    \n",
    "    # Floor column: normalize floor values and handle special cases\n",
    "    if column_info['floor_col']:\n",
    "        floor_pipeline = Pipeline([\n",
    "            ('normalize_floor', FloorNormalizer()),\n",
    "            ('impute', SimpleImputer(strategy='median'))\n",
    "        ])\n",
    "        transformers.append(('floor', floor_pipeline, [column_info['floor_col']]))\n",
    "    \n",
    "    # Categorical columns: one-hot encode with handling for unknown values\n",
    "    if column_info['categorical_cols']:\n",
    "        categorical_pipeline = Pipeline([\n",
    "            ('impute', SimpleImputer(strategy='constant', fill_value='Unknown')),\n",
    "            # Convert all values to strings to ensure uniform type\n",
    "            ('to_string', FunctionTransformer(lambda X: X.astype(str))),\n",
    "            ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, drop='first'))\n",
    "        ])\n",
    "        transformers.append(('categorical', categorical_pipeline, column_info['categorical_cols']))\n",
    "    \n",
    "    # Boolean columns: convert to int\n",
    "    if column_info['bool_cols']:\n",
    "        bool_pipeline = Pipeline([\n",
    "            ('to_int', FunctionTransformer(lambda X: X.astype(int))),\n",
    "            ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "        ])\n",
    "        transformers.append(('boolean', bool_pipeline, column_info['bool_cols']))\n",
    "    \n",
    "    # Binary feature columns: already binary, just impute missing values\n",
    "    if column_info['binary_features_cols']:\n",
    "        binary_features_pipeline = Pipeline([\n",
    "            ('impute', SimpleImputer(strategy='most_frequent'))\n",
    "        ])\n",
    "        transformers.append(('binary_features', binary_features_pipeline, column_info['binary_features_cols']))\n",
    "    \n",
    "    # Create the final preprocessor\n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=transformers,\n",
    "        remainder='drop'  # Drop any columns not explicitly listed\n",
    "    )\n",
    "    \n",
    "    return preprocessor, column_info\n",
    "\n",
    "# Create the pipeline based on our current dataframe\n",
    "preprocessor, column_info = create_preprocessing_pipeline(df_subset)\n",
    "\n",
    "# Now we have a reusable preprocessor that can be:\n",
    "# 1. Fit on training data: preprocessor.fit(X_train)\n",
    "# 2. Used to transform both train and test: X_train_processed = preprocessor.transform(X_train)\n",
    "# 3. Saved to disk for later use: pickle.dump((preprocessor, column_info), open('preprocessor.pkl', 'wb'))\n",
    "# 4. Loaded and applied to new data: pickle.load(open('preprocessor.pkl', 'rb'))[0].transform(new_data)\n",
    "\n",
    "# Let's demonstrate how to use it with our current data\n",
    "print(\"Using the preprocessing pipeline:\")\n",
    "\n",
    "# Create a copy of our data for demonstration\n",
    "X = df_subset.copy()\n",
    "y = None\n",
    "if 'price_value' in X.columns:\n",
    "    y = X['price_value']\n",
    "    X = X.drop('price_value', axis=1)\n",
    "\n",
    "# Show some sample rows before preprocessing\n",
    "print(\"\\nBefore preprocessing (first 3 rows, selected columns):\")\n",
    "display_cols = ['surface', 'rooms']\n",
    "if 'floor_number' in X.columns:\n",
    "    display_cols.append('floor_number')\n",
    "elif 'floor' in X.columns:\n",
    "    display_cols.append('floor')\n",
    "if 'city' in X.columns:\n",
    "    display_cols.append('city')\n",
    "print(X[display_cols].head(3))\n",
    "\n",
    "# Fit and transform\n",
    "X_transformed = preprocessor.fit_transform(X)\n",
    "\n",
    "# Access column information from our stored dictionary\n",
    "numeric_cols = column_info['numeric_cols']\n",
    "floor_col = column_info['floor_col']\n",
    "categorical_cols = column_info['categorical_cols']\n",
    "bool_cols = column_info['bool_cols']\n",
    "binary_features_cols = column_info['binary_features_cols']\n",
    "\n",
    "# Get feature names for the transformed data using the number of output features\n",
    "# This ensures we have the correct number of names\n",
    "n_features = X_transformed.shape[1]\n",
    "print(f\"Transformed data has {n_features} features\")\n",
    "\n",
    "# Create generic feature names if needed\n",
    "feature_names = [f\"feature_{i}\" for i in range(n_features)]\n",
    "\n",
    "# Try to get actual feature names where possible\n",
    "feature_index = 0\n",
    "numeric_cols_out = []\n",
    "if 'numeric' in preprocessor.named_transformers_ and len(numeric_cols) > 0:\n",
    "    numeric_cols_out = numeric_cols\n",
    "    for i, col in enumerate(numeric_cols):\n",
    "        if feature_index < n_features:\n",
    "            feature_names[feature_index] = col\n",
    "            feature_index += 1\n",
    "\n",
    "floor_col_out = []\n",
    "if 'floor' in preprocessor.named_transformers_ and floor_col is not None:\n",
    "    floor_col_out = ['floor_numeric']\n",
    "    if feature_index < n_features:\n",
    "        feature_names[feature_index] = 'floor_numeric'\n",
    "        feature_index += 1\n",
    "\n",
    "cat_cols_out = []\n",
    "if 'categorical' in preprocessor.named_transformers_ and len(categorical_cols) > 0:\n",
    "    ohe = preprocessor.named_transformers_['categorical'].named_steps['onehot']\n",
    "    cat_cols_out = ohe.get_feature_names_out(categorical_cols).tolist()\n",
    "    for i, col in enumerate(cat_cols_out):\n",
    "        if feature_index < n_features:\n",
    "            feature_names[feature_index] = col\n",
    "            feature_index += 1\n",
    "\n",
    "bool_cols_out = []\n",
    "if 'boolean' in preprocessor.named_transformers_ and len(bool_cols) > 0:\n",
    "    bool_cols_out = bool_cols\n",
    "    for i, col in enumerate(bool_cols):\n",
    "        if feature_index < n_features:\n",
    "            feature_names[feature_index] = col\n",
    "            feature_index += 1\n",
    "\n",
    "binary_features_cols_out = []\n",
    "if 'binary_features' in preprocessor.named_transformers_ and len(binary_features_cols) > 0:\n",
    "    binary_features_cols_out = binary_features_cols\n",
    "    for i, col in enumerate(binary_features_cols):\n",
    "        if feature_index < n_features:\n",
    "            feature_names[feature_index] = col\n",
    "            feature_index += 1\n",
    "\n",
    "# Convert to DataFrame to show the transformed data\n",
    "X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
    "print(\"\\nAfter preprocessing (first 3 rows, selected features):\")\n",
    "print(X_transformed_df.head(3))\n",
    "\n",
    "print(f\"\\nTransformed data shape: {X_transformed.shape} with {len(feature_names)} features\")\n",
    "print(f\"Number of feature types: {len(numeric_cols_out)} numeric, {len(floor_col_out)} floor, \" \n",
    "      f\"{len(cat_cols_out)} categorical, {len(bool_cols_out)} boolean, {len(binary_features_cols_out)} binary features\")\n",
    "\n",
    "# Example of saving the pipeline to disk\n",
    "# pickle.dump((preprocessor, column_info), open('preprocessor.pkl', 'wb'))\n",
    "# print(\"\\nPreprocessor saved to 'preprocessor.pkl'\")\n",
    "\n",
    "# Example of how you would use this with new data for prediction\n",
    "print(\"\\nExample of using the pipeline with new data:\")\n",
    "print(\"# Load the pipeline and column info\")\n",
    "print(\"preprocessor, column_info = pickle.load(open('preprocessor.pkl', 'rb'))\")\n",
    "print(\"\")\n",
    "print(\"# Create new data with the same features as used during training\")\n",
    "print(\"new_data = pd.DataFrame([{\")\n",
    "print(\"    'surface': '120', \")\n",
    "print(\"    'rooms': '3', \")\n",
    "print(f\"    '{column_info['floor_col']}': 'piano terra', \")\n",
    "print(\"    'city': 'Genova',\")\n",
    "print(\"    # Add all other features that were used during training...\")\n",
    "print(\"}])\")\n",
    "print(\"\")\n",
    "print(\"# Make prediction\")\n",
    "print(\"preprocessed_features = preprocessor.transform(new_data)  # Returns preprocessed features ready for the model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701c7cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and evaluate ML models using our cleaned dataset\n",
    "\n",
    "# Drop rows with any remaining NaN values for training\n",
    "df_ml = df_clean.dropna()\n",
    "print(f\"Dataset shape after dropping NaN values: {df_ml.shape}\")\n",
    "\n",
    "# Check and remove any non-numeric columns before modeling\n",
    "numeric_columns = df_ml.select_dtypes(include=['number']).columns.tolist()\n",
    "print(f\"Number of numeric columns: {len(numeric_columns)}\")\n",
    "\n",
    "if 'price_value' not in numeric_columns:\n",
    "    print(\"Warning: price_value column is not numeric!\")\n",
    "else:\n",
    "    # Separate features and target\n",
    "    X = df_ml[numeric_columns].drop('price_value', axis=1)\n",
    "    y = df_ml['price_value']\n",
    "    \n",
    "    print(\"Data types in features:\")\n",
    "    print(X.dtypes.value_counts())\n",
    "\n",
    "    # Split the data using parameters from config\n",
    "    test_size = CONFIG['ml']['test_size']\n",
    "    random_state = CONFIG['ml']['random_state']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "    # Define a function to evaluate models\n",
    "    def evaluate_model(model, X_train, X_test, y_train, y_test):\n",
    "        # Fit model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        \n",
    "        print(f\"Mean Absolute Error: €{mae:.2f}\")\n",
    "        print(f\"Root Mean Squared Error: €{rmse:.2f}\")\n",
    "        print(f\"R² Score: {r2:.4f}\")\n",
    "        \n",
    "        return model, mae, rmse, r2\n",
    "\n",
    "    # Train and evaluate multiple models\n",
    "    # Scale the features for linear models\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Linear models (using scaled data)\n",
    "print(\"\\n--- Linear Regression ---\")\n",
    "lr_model, lr_mae, lr_rmse, lr_r2 = evaluate_model(\n",
    "    LinearRegression(), X_train_scaled, X_test_scaled, y_train, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n--- Ridge Regression ---\")\n",
    "ridge_model, ridge_mae, ridge_rmse, ridge_r2 = evaluate_model(\n",
    "    Ridge(alpha=1.0), X_train_scaled, X_test_scaled, y_train, y_test\n",
    ")\n",
    "\n",
    "# Get n_estimators from config\n",
    "n_estimators = CONFIG['ml']['n_estimators']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "\n",
    "# Tree-based models (don't need scaling)\n",
    "print(\"\\n--- Random Forest ---\")\n",
    "rf_model, rf_mae, rf_rmse, rf_r2 = evaluate_model(\n",
    "    RandomForestRegressor(n_estimators=n_estimators, random_state=random_state), X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "print(\"\\n--- Gradient Boosting ---\")\n",
    "gb_model, gb_mae, gb_rmse, gb_r2 = evaluate_model(\n",
    "    GradientBoostingRegressor(n_estimators=n_estimators, random_state=random_state), X_train, X_test, y_train, y_test\n",
    ")\n",
    "\n",
    "# Compare model performance\n",
    "models = ['Linear Regression', 'Ridge Regression', 'Random Forest', 'Gradient Boosting']\n",
    "r2_scores = [lr_r2, ridge_r2, rf_r2, gb_r2]\n",
    "mae_scores = [lr_mae, ridge_mae, rf_mae, gb_mae]\n",
    "\n",
    "# Find best model based on R² score\n",
    "best_model_idx = np.argmax(r2_scores)\n",
    "print(f\"\\nBest model based on R² score: {models[best_model_idx]} (R² = {r2_scores[best_model_idx]:.4f})\")\n",
    "\n",
    "# If we have RandomForest or GradientBoosting, check feature importance\n",
    "if best_model_idx >= 2:  # Index 2 or 3 (tree-based models)\n",
    "    best_model = [lr_model, ridge_model, rf_model, gb_model][best_model_idx]\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(feature_importance['Feature'].head(10)[::-1], \n",
    "             feature_importance['Importance'].head(10)[::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 10 Most Important Features ({models[best_model_idx]})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b2b750",
   "metadata": {},
   "source": [
    "# Building Models with the Pipeline\n",
    "\n",
    "Now we'll use our preprocessing pipeline to build a complete model pipeline that includes:\n",
    "1. The preprocessing steps from above\n",
    "2. The model training steps\n",
    "3. Evaluation metrics\n",
    "\n",
    "This approach ensures that the entire workflow from raw data to predictions is encapsulated in a single pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86250cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data: X and y from our subset\n",
    "X = df_subset.copy()\n",
    "if 'price_value' in X.columns:\n",
    "    y = X.pop('price_value')  # Remove and get the target\n",
    "else:\n",
    "    raise ValueError(\"Target 'price_value' not found in the dataframe\")\n",
    "\n",
    "# Check for and remove NaN values\n",
    "nan_mask = y.notna()\n",
    "X = X[nan_mask]\n",
    "y = y[nan_mask]\n",
    "\n",
    "print(f\"Removed {sum(~nan_mask)} rows with NaN target values\")\n",
    "print(f\"Shape after removing NaN values: X: {X.shape}, y: {y.shape}\")\n",
    "\n",
    "# Use train_test_split parameters from config\n",
    "test_size = CONFIG['ml']['test_size']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "\n",
    "# Create a function to build a complete pipeline with preprocessing and model\n",
    "def build_model_pipeline(model, preprocessor=None, column_info=None):\n",
    "    \"\"\"\n",
    "    Builds a complete sklearn pipeline with preprocessing and model.\n",
    "    \n",
    "    Args:\n",
    "        model: A scikit-learn model/estimator\n",
    "        preprocessor: A fitted or unfitted preprocessing pipeline\n",
    "        column_info: Dictionary with column information\n",
    "        \n",
    "    Returns:\n",
    "        A complete pipeline\n",
    "    \"\"\"\n",
    "    if preprocessor is None:\n",
    "        # Create a new preprocessor if one isn't provided\n",
    "        preprocessor, column_info = create_preprocessing_pipeline()\n",
    "    \n",
    "    # Create a pipeline that first preprocesses the data, then fits the model\n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    return pipeline, column_info\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate_model_pipeline(pipeline, X_train, X_test, y_train, y_test, model_name=\"Model\"):\n",
    "    \"\"\"\n",
    "    Fits a model pipeline and evaluates its performance.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: A scikit-learn pipeline with preprocessing and model\n",
    "        X_train: Training features\n",
    "        X_test: Test features\n",
    "        y_train: Training target\n",
    "        y_test: Test target\n",
    "        model_name: Name of the model for display\n",
    "        \n",
    "    Returns:\n",
    "        The fitted pipeline and performance metrics\n",
    "    \"\"\"\n",
    "    # Fit the pipeline on training data\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    \n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_pred_train))\n",
    "    test_rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n--- {model_name} ---\")\n",
    "    print(f\"Training MAE: €{train_mae:.2f}, Test MAE: €{test_mae:.2f}\")\n",
    "    print(f\"Training RMSE: €{train_rmse:.2f}, Test RMSE: €{test_rmse:.2f}\")\n",
    "    print(f\"Training R²: {train_r2:.4f}, Test R²: {test_r2:.4f}\")\n",
    "    \n",
    "    # Return the fitted pipeline and metrics\n",
    "    return pipeline, {\n",
    "        'name': model_name,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "\n",
    "# Create a shared preprocessor for all models\n",
    "preprocessor, column_info = create_preprocessing_pipeline(X)\n",
    "\n",
    "# Get n_estimators from config\n",
    "n_estimators = CONFIG['ml']['n_estimators']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "\n",
    "# Define models\n",
    "models = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Ridge Regression', Ridge(alpha=1.0)),\n",
    "    ('Random Forest', RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)),\n",
    "    ('Gradient Boosting', GradientBoostingRegressor(n_estimators=n_estimators, random_state=random_state))\n",
    "]\n",
    "\n",
    "# Train and evaluate all models\n",
    "results = []\n",
    "trained_pipelines = {}\n",
    "\n",
    "for name, model in models:\n",
    "    pipeline, _ = build_model_pipeline(model, preprocessor, column_info)\n",
    "    trained_pipeline, metrics = evaluate_model_pipeline(\n",
    "        pipeline, X_train, X_test, y_train, y_test, name\n",
    "    )\n",
    "    results.append(metrics)\n",
    "    trained_pipelines[name] = trained_pipeline\n",
    "\n",
    "# Compare model performance with a visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "best_model = results_df.loc[results_df['test_r2'].idxmax()]\n",
    "\n",
    "print(f\"\\nBest model based on Test R² score: {best_model['name']} (R² = {best_model['test_r2']:.4f})\")\n",
    "\n",
    "# Plot R² scores\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "bars = plt.bar(results_df['name'], results_df['test_r2'], color='skyblue')\n",
    "plt.title('Test R² Scores by Model')\n",
    "plt.ylabel('R² Score (higher is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Highlight the best model\n",
    "best_idx = results_df['test_r2'].idxmax()\n",
    "bars[best_idx].set_color('darkblue')\n",
    "\n",
    "# Plot MAE scores\n",
    "plt.subplot(1, 2, 2)\n",
    "bars = plt.bar(results_df['name'], results_df['test_mae'], color='lightcoral')\n",
    "plt.title('Test MAE Scores by Model')\n",
    "plt.ylabel('Mean Absolute Error € (lower is better)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Highlight the best model by MAE\n",
    "best_mae_idx = results_df['test_mae'].idxmin()\n",
    "bars[best_mae_idx].set_color('darkred')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# If the best model is Random Forest or Gradient Boosting, check feature importance\n",
    "best_model_name = best_model['name']\n",
    "if best_model_name in ['Random Forest', 'Gradient Boosting']:\n",
    "    # Get the trained model from the pipeline\n",
    "    model_step = trained_pipelines[best_model_name].named_steps['model']\n",
    "    \n",
    "    # Get the preprocessor to get feature names\n",
    "    preprocessor_step = trained_pipelines[best_model_name].named_steps['preprocessor']\n",
    "    \n",
    "    # Extract feature names from the column information\n",
    "    feature_names = []\n",
    "    \n",
    "    # Add numeric features\n",
    "    if column_info['numeric_cols']:\n",
    "        feature_names.extend(column_info['numeric_cols'])\n",
    "    \n",
    "    # Add floor feature\n",
    "    if column_info['floor_col'] is not None:\n",
    "        feature_names.append('floor_numeric')\n",
    "    \n",
    "    # Add categorical features (one-hot encoded)\n",
    "    if column_info['categorical_cols']:\n",
    "        categorical_transformer = preprocessor_step.named_transformers_.get('categorical')\n",
    "        if categorical_transformer:\n",
    "            ohe = categorical_transformer.named_steps['onehot']\n",
    "            categorical_features = ohe.get_feature_names_out(column_info['categorical_cols']).tolist()\n",
    "            feature_names.extend(categorical_features)\n",
    "    \n",
    "    # Add boolean features\n",
    "    if column_info['bool_cols']:\n",
    "        feature_names.extend(column_info['bool_cols'])\n",
    "    \n",
    "    # Add binary features from ga4features extraction\n",
    "    if 'binary_features_cols' in column_info and column_info['binary_features_cols']:\n",
    "        feature_names.extend(column_info['binary_features_cols'])\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': model_step.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importance:\")\n",
    "    print(feature_importance.head(15))  # Show top 15 instead of 10 to see more binary features\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))  # Larger figure to accommodate more features\n",
    "    plt.barh(feature_importance['Feature'].head(15)[::-1], \n",
    "             feature_importance['Importance'].head(15)[::-1])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.title(f'Top 15 Most Important Features ({best_model_name})')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Specifically analyze the importance of the binary features from ga4features\n",
    "    if 'binary_features_cols' in column_info and column_info['binary_features_cols']:\n",
    "        binary_features_importance = feature_importance[\n",
    "            feature_importance['Feature'].isin(column_info['binary_features_cols'])\n",
    "        ].sort_values('Importance', ascending=False)\n",
    "        \n",
    "        print(\"\\nImportance of Property Features (ga4features):\")\n",
    "        print(binary_features_importance)\n",
    "        \n",
    "        # Plot just the binary features importance if there are enough\n",
    "        if len(binary_features_importance) > 3:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.barh(binary_features_importance['Feature'].head(10)[::-1], \n",
    "                     binary_features_importance['Importance'].head(10)[::-1])\n",
    "            plt.xlabel('Importance')\n",
    "            plt.title(f'Property Features Importance ({best_model_name})')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Save the best model for future use\n",
    "best_pipeline = trained_pipelines[best_model_name]\n",
    "# Save both the pipeline and column_info for later use\n",
    "# pickle.dump({'pipeline': best_pipeline, 'column_info': column_info}, \n",
    "#           open(f'best_model_{best_model_name.replace(\" \", \"_\").lower()}.pkl', 'wb'))\n",
    "print(f\"\\nBest model pipeline is ready for use. You can save it using:\")\n",
    "print(f\"pickle.dump({{'pipeline': best_pipeline, 'column_info': column_info}}, open('best_model.pkl', 'wb'))\")\n",
    "\n",
    "# Example of how to use the saved model for prediction\n",
    "print(\"\\nExample of using the saved model for prediction:\")\n",
    "print(\"# Load the model\")\n",
    "print(\"saved_data = pickle.load(open('best_model.pkl', 'rb'))\")\n",
    "print(\"model = saved_data['pipeline']\")\n",
    "print(\"column_info = saved_data['column_info']\")\n",
    "print(\"\")\n",
    "print(\"# Create new data with the same features as the training data\")\n",
    "print(\"new_data = pd.DataFrame([{\")\n",
    "print(\"    'surface': '120', \")\n",
    "print(\"    'rooms': '3', \")\n",
    "print(f\"    '{column_info['floor_col']}': 'piano terra', \")\n",
    "print(\"    'city': 'Genova',\")\n",
    "print(\"    'has_arredato': 1,  # Example of binary feature\")\n",
    "print(\"    'has_balcone': 1,   # Example of binary feature\")\n",
    "print(\"    # Add all other features that were used during training...\")\n",
    "print(\"}])\")\n",
    "print(\"\")\n",
    "print(\"# Make prediction\")\n",
    "print(\"prediction = model.predict(new_data)\")\n",
    "print(\"print(f'Predicted price: €{prediction[0]:.2f}')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e078bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c20b39ac",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Let's add the ability to select specific features for our model. This can help:\n",
    "1. Reduce model complexity and improve interpretability\n",
    "2. Focus on the most important predictors\n",
    "3. Remove redundant or irrelevant features\n",
    "4. Create simpler models that may generalize better\n",
    "\n",
    "We'll implement both manual feature selection and automated feature selection methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f049d82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ipywidgets\n",
    "\n",
    "# First, let's explore all available features in our dataset\n",
    "X = df_subset.copy()\n",
    "if 'price_value' in X.columns:\n",
    "    y = X.pop('price_value')  # Remove the target variable\n",
    "else:\n",
    "    raise ValueError(\"Target 'price_value' not found in the dataframe\")\n",
    "\n",
    "# Get a clean mask for non-NaN values in the target\n",
    "nan_mask = y.notna()\n",
    "X = X[nan_mask]\n",
    "y = y[nan_mask]\n",
    "\n",
    "# Get the preprocessor to extract feature names\n",
    "preprocessor, column_info = create_preprocessing_pipeline(X)\n",
    "\n",
    "# Initialize lists to store different types of features\n",
    "all_features = []\n",
    "\n",
    "# Add numeric features\n",
    "numeric_features = []\n",
    "if column_info['numeric_cols']:\n",
    "    numeric_features = column_info['numeric_cols']\n",
    "    all_features.extend(numeric_features)\n",
    "    \n",
    "# Add floor feature\n",
    "floor_feature = None\n",
    "if column_info['floor_col'] is not None:\n",
    "    floor_feature = 'floor_numeric'\n",
    "    all_features.append(floor_feature)\n",
    "    \n",
    "# Add categorical features\n",
    "categorical_features = []\n",
    "if column_info['categorical_cols']:\n",
    "    categorical_features = column_info['categorical_cols']\n",
    "    all_features.extend(categorical_features)\n",
    "    \n",
    "# Add boolean features\n",
    "boolean_features = []\n",
    "if column_info['bool_cols']:\n",
    "    boolean_features = column_info['bool_cols']\n",
    "    all_features.extend(boolean_features)\n",
    "    \n",
    "# Add binary features from ga4features\n",
    "binary_features = []\n",
    "if 'binary_features_cols' in column_info and column_info['binary_features_cols']:\n",
    "    binary_features = column_info['binary_features_cols']\n",
    "    all_features.extend(binary_features)\n",
    "\n",
    "print(f\"Total number of potential features: {len(all_features)}\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}):\")\n",
    "print(numeric_features)\n",
    "\n",
    "print(f\"\\nFloor feature:\")\n",
    "print(floor_feature)\n",
    "\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}):\")\n",
    "print(categorical_features)\n",
    "\n",
    "print(f\"\\nBoolean features ({len(boolean_features)}):\")\n",
    "print(boolean_features)\n",
    "\n",
    "print(f\"\\nBinary features from property characteristics ({len(binary_features)}):\")\n",
    "print(binary_features[:10])  # Show first 10 to avoid overwhelming output\n",
    "if len(binary_features) > 10:\n",
    "    print(f\"... and {len(binary_features) - 10} more binary features\")\n",
    "\n",
    "# Create feature selection widgets\n",
    "feature_groups = {\n",
    "    \"Numeric Features\": numeric_features,\n",
    "    \"Floor Feature\": [floor_feature] if floor_feature else [],\n",
    "    \"Categorical Features\": categorical_features,\n",
    "    \"Boolean Features\": boolean_features,\n",
    "    \"Binary Property Features\": binary_features\n",
    "}\n",
    "\n",
    "# Dictionary to store the selected features\n",
    "selected_features = {}\n",
    "\n",
    "# Create UI for each feature group\n",
    "for group_name, features in feature_groups.items():\n",
    "    if not features:  # Skip empty feature groups\n",
    "        continue\n",
    "        \n",
    "    print(f\"\\n## {group_name}\")\n",
    "    \n",
    "    # Create a checkbox widget for each feature\n",
    "    checkboxes = []\n",
    "    for feature in features:\n",
    "        checkbox = widgets.Checkbox(\n",
    "            value=True,  # Default to selected\n",
    "            description=feature,\n",
    "            disabled=False\n",
    "        )\n",
    "        checkboxes.append((feature, checkbox))\n",
    "        display(checkbox)\n",
    "    \n",
    "    # Store checkboxes for later access\n",
    "    selected_features[group_name] = checkboxes\n",
    "\n",
    "# Create a button to finalize feature selection\n",
    "select_button = widgets.Button(\n",
    "    description='Apply Feature Selection',\n",
    "    button_style='success',\n",
    "    tooltip='Click to apply the selected features',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Function to handle button click\n",
    "def on_select_button_clicked(b):\n",
    "    # Collect all selected features\n",
    "    final_selected_features = []\n",
    "    \n",
    "    for group_name, feature_checkboxes in selected_features.items():\n",
    "        for feature, checkbox in feature_checkboxes:\n",
    "            if checkbox.value:\n",
    "                final_selected_features.append(feature)\n",
    "    \n",
    "    print(f\"\\nSelected {len(final_selected_features)} features for modeling:\")\n",
    "    print(final_selected_features)\n",
    "    \n",
    "    # Store in a global variable for use in next cells\n",
    "    global user_selected_features\n",
    "    user_selected_features = final_selected_features\n",
    "    \n",
    "    # Create a modified version of the preprocessing pipeline that only includes selected features\n",
    "    global user_selected_column_info\n",
    "    user_selected_column_info = {\n",
    "        'numeric_cols': [f for f in numeric_features if f in final_selected_features],\n",
    "        'floor_col': column_info['floor_col'] if floor_feature in final_selected_features else None,\n",
    "        'categorical_cols': [f for f in categorical_features if f in final_selected_features],\n",
    "        'bool_cols': [f for f in boolean_features if f in final_selected_features],\n",
    "        'binary_features_cols': [f for f in binary_features if f in final_selected_features]\n",
    "    }\n",
    "    \n",
    "    print(\"\\nFeature selection applied successfully!\")\n",
    "    print(\"You can now train models with these features in the next cell.\")\n",
    "\n",
    "select_button.on_click(on_select_button_clicked)\n",
    "display(select_button)\n",
    "\n",
    "# Provide instructions\n",
    "display(HTML(\"<p><i>Select the features you want to include in your model, then click 'Apply Feature Selection'</i></p>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b56619d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automated Feature Selection Methods\n",
    "\n",
    "# Make sure we have the preprocessed data\n",
    "X = df_subset.copy()\n",
    "if 'price_value' in X.columns:\n",
    "    y = X.pop('price_value')\n",
    "else:\n",
    "    raise ValueError(\"Target 'price_value' not found in the dataframe\")\n",
    "\n",
    "# Handle NaN values\n",
    "nan_mask = y.notna()\n",
    "X = X[nan_mask]\n",
    "y = y[nan_mask]\n",
    "\n",
    "# Split data using parameters from config\n",
    "test_size = CONFIG['ml']['test_size']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "# Create a preprocessor\n",
    "preprocessor, column_info = create_preprocessing_pipeline(X)\n",
    "\n",
    "# Fit and transform training data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Get feature names\n",
    "feature_names = []\n",
    "\n",
    "# Add numeric features\n",
    "if column_info['numeric_cols']:\n",
    "    feature_names.extend(column_info['numeric_cols'])\n",
    "\n",
    "# Add floor feature\n",
    "if column_info['floor_col'] is not None:\n",
    "    feature_names.append('floor_numeric')\n",
    "\n",
    "# Add categorical features (one-hot encoded)\n",
    "if column_info['categorical_cols']:\n",
    "    categorical_transformer = preprocessor.named_transformers_.get('categorical')\n",
    "    if categorical_transformer:\n",
    "        ohe = categorical_transformer.named_steps['onehot']\n",
    "        cat_features = ohe.get_feature_names_out(column_info['categorical_cols']).tolist()\n",
    "        feature_names.extend(cat_features)\n",
    "\n",
    "# Add boolean features\n",
    "if column_info['bool_cols']:\n",
    "    feature_names.extend(column_info['bool_cols'])\n",
    "\n",
    "# Add binary features\n",
    "if 'binary_features_cols' in column_info and column_info['binary_features_cols']:\n",
    "    feature_names.extend(column_info['binary_features_cols'])\n",
    "\n",
    "print(f\"Number of features after preprocessing: {len(feature_names)}\")\n",
    "print(f\"Shape of X_train_processed: {X_train_processed.shape}\")\n",
    "\n",
    "# Get top_features_to_select from config\n",
    "top_n_features = CONFIG['ml']['top_features_to_select']\n",
    "\n",
    "# Method 1: Correlation with Target\n",
    "print(\"\\n## Method 1: Correlation with Target (F-regression)\")\n",
    "selector = SelectKBest(score_func=f_regression, k=min(top_n_features, len(feature_names)))\n",
    "X_new = selector.fit_transform(X_train_processed, y_train)\n",
    "\n",
    "# Get scores and p-values\n",
    "scores = selector.scores_\n",
    "p_values = selector.pvalues_\n",
    "\n",
    "# Debug: Check lengths of arrays\n",
    "print(f\"Length of feature_names: {len(feature_names)}\")\n",
    "print(f\"Length of scores: {len(scores)}\")\n",
    "print(f\"Length of p_values: {len(p_values)}\")\n",
    "\n",
    "# Make sure feature_names matches the number of features in X_train_processed\n",
    "if len(feature_names) != X_train_processed.shape[1]:\n",
    "    print(\"Warning: Feature names length doesn't match the number of features in the processed data!\")\n",
    "    # If more feature names than actual features, truncate feature_names\n",
    "    if len(feature_names) > X_train_processed.shape[1]:\n",
    "        feature_names = feature_names[:X_train_processed.shape[1]]\n",
    "        print(f\"Truncated feature_names to match X_train_processed shape: {len(feature_names)}\")\n",
    "    # If fewer feature names than actual features, add generic names\n",
    "    else:\n",
    "        additional_features = X_train_processed.shape[1] - len(feature_names)\n",
    "        feature_names.extend([f\"unknown_feature_{i}\" for i in range(additional_features)])\n",
    "        print(f\"Added {additional_features} generic feature names to match X_train_processed shape\")\n",
    "\n",
    "# Create DataFrame with feature scores\n",
    "feature_scores = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores,\n",
    "    'P-value': p_values\n",
    "})\n",
    "\n",
    "# Sort by score in descending order\n",
    "feature_scores = feature_scores.sort_values('Score', ascending=False)\n",
    "\n",
    "print(f\"\\nTop {top_n_features} features by correlation with target:\")\n",
    "print(feature_scores.head(top_n_features))\n",
    "\n",
    "# Plot top features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Score', y='Feature', data=feature_scores.head(top_n_features))\n",
    "plt.title(f'Top {top_n_features} Features by F-regression Score')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Method 2: Recursive Feature Elimination (RFE)\n",
    "print(\"\\n## Method 2: Recursive Feature Elimination (RFE)\")\n",
    "n_features_to_select = min(top_n_features, len(feature_names))\n",
    "n_estimators = CONFIG['ml']['n_estimators']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "estimator = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "rfe = RFE(estimator, n_features_to_select=n_features_to_select)\n",
    "\n",
    "# Fit RFE\n",
    "rfe.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get selected features\n",
    "selected_mask = rfe.support_\n",
    "ranking = rfe.ranking_\n",
    "\n",
    "# Create DataFrame with feature ranking\n",
    "feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Selected': selected_mask,\n",
    "    'Ranking': ranking\n",
    "})\n",
    "\n",
    "# Sort by ranking (ascending = better)\n",
    "feature_ranking = feature_ranking.sort_values('Ranking')\n",
    "\n",
    "print(\"\\nTop features selected by RFE:\")\n",
    "print(feature_ranking[feature_ranking['Selected']].reset_index(drop=True))\n",
    "\n",
    "# Plot ranking\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Ranking', y='Feature', \n",
    "            data=feature_ranking.sort_values('Ranking').head(top_n_features))\n",
    "plt.title('Feature Ranking by RFE (lower is better)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Method 3: Feature Importance from Random Forest\n",
    "print(\"\\n## Method 3: Feature Importance from Random Forest\")\n",
    "n_estimators = CONFIG['ml']['n_estimators']\n",
    "random_state = CONFIG['ml']['random_state']\n",
    "rf = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n",
    "rf.fit(X_train_processed, y_train)\n",
    "\n",
    "# Get feature importances\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# Create DataFrame with feature importances\n",
    "feature_importances = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Sort by importance in descending order\n",
    "feature_importances = feature_importances.sort_values('Importance', ascending=False)\n",
    "\n",
    "print(f\"\\nTop {top_n_features} features by Random Forest importance:\")\n",
    "print(feature_importances.head(top_n_features))\n",
    "\n",
    "# Plot importances\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances.head(top_n_features))\n",
    "plt.title('Feature Importance from Random Forest')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a button to choose top features automatically\n",
    "top_n_slider = widgets.IntSlider(\n",
    "    value=top_n_features,\n",
    "    min=5,\n",
    "    max=min(50, len(feature_names)),\n",
    "    step=1,\n",
    "    description='Top N features:',\n",
    "    disabled=False,\n",
    "    continuous_update=False,\n",
    "    orientation='horizontal',\n",
    "    readout=True,\n",
    "    readout_format='d'\n",
    ")\n",
    "\n",
    "method_dropdown = widgets.Dropdown(\n",
    "    options=['Correlation (F-regression)', 'RFE', 'Random Forest Importance'],\n",
    "    value='Random Forest Importance',\n",
    "    description='Method:',\n",
    "    disabled=False,\n",
    ")\n",
    "\n",
    "auto_select_button = widgets.Button(\n",
    "    description='Select Top Features',\n",
    "    button_style='primary',\n",
    "    tooltip='Click to select top features using the chosen method',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "display(method_dropdown)\n",
    "display(top_n_slider)\n",
    "display(auto_select_button)\n",
    "\n",
    "# Function to handle automatic selection\n",
    "def on_auto_select_clicked(b):\n",
    "    method = method_dropdown.value\n",
    "    n = top_n_slider.value\n",
    "    \n",
    "    if method == 'Correlation (F-regression)':\n",
    "        top_features = feature_scores.head(n)['Feature'].tolist()\n",
    "    elif method == 'RFE':\n",
    "        top_features = feature_ranking.head(n)['Feature'].tolist()\n",
    "    else:  # Random Forest Importance\n",
    "        top_features = feature_importances.head(n)['Feature'].tolist()\n",
    "    \n",
    "    print(f\"\\nAutomatically selected top {n} features using {method}:\")\n",
    "    print(top_features)\n",
    "    \n",
    "    # Store in a global variable for use in next cells\n",
    "    global auto_selected_features\n",
    "    auto_selected_features = top_features\n",
    "    \n",
    "    # Create a modified version of the column info\n",
    "    global auto_selected_column_info\n",
    "    auto_selected_column_info = {\n",
    "        'numeric_cols': [f for f in column_info['numeric_cols'] if f in top_features],\n",
    "        'floor_col': column_info['floor_col'] if 'floor_numeric' in top_features else None,\n",
    "        'categorical_cols': [],  # We'll handle these differently as they're one-hot encoded\n",
    "        'bool_cols': [f for f in column_info['bool_cols'] if f in top_features],\n",
    "        'binary_features_cols': [f for f in column_info.get('binary_features_cols', []) if f in top_features]\n",
    "    }\n",
    "    \n",
    "    # Handle categorical features (need to match original feature names before one-hot encoding)\n",
    "    for cat_feature in column_info['categorical_cols']:\n",
    "        for top_feature in top_features:\n",
    "            if top_feature.startswith(cat_feature + '_'):\n",
    "                if cat_feature not in auto_selected_column_info['categorical_cols']:\n",
    "                    auto_selected_column_info['categorical_cols'].append(cat_feature)\n",
    "    \n",
    "    print(\"\\nAutomatic feature selection applied successfully!\")\n",
    "    print(\"You can now train models with these features in the next cell.\")\n",
    "\n",
    "auto_select_button.on_click(on_auto_select_clicked)\n",
    "\n",
    "# Display help text\n",
    "display(HTML(\"\"\"<p><i>Choose the feature selection method and number of features to include, \n",
    "then click 'Select Top Features'</i></p>\"\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d99f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model with selected features\n",
    "\n",
    "# Function to train and evaluate models with selected features\n",
    "def train_model_with_selected_features(selected_column_info, selection_method=\"user\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using only the selected features\n",
    "    \n",
    "    Args:\n",
    "        selected_column_info: Dictionary with information about selected columns\n",
    "        selection_method: String describing the selection method (for display purposes)\n",
    "    \"\"\"\n",
    "    print(f\"\\n## Training models with {selection_method} selected features\")\n",
    "    \n",
    "    # Prepare data\n",
    "    X = df_subset.copy()\n",
    "    if 'price_value' in X.columns:\n",
    "        y = X.pop('price_value')\n",
    "    else:\n",
    "        raise ValueError(\"Target 'price_value' not found in the dataframe\")\n",
    "\n",
    "    # Handle NaN values\n",
    "    nan_mask = y.notna()\n",
    "    X = X[nan_mask]\n",
    "    y = y[nan_mask]\n",
    "\n",
    "    # Split data using parameters from config\n",
    "    test_size = CONFIG['ml']['test_size']\n",
    "    random_state = CONFIG['ml']['random_state']\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    # Create a custom preprocessor with selected features\n",
    "    preprocessor = create_preprocessing_pipeline(X)[0]  # Reuse existing function but extract only preprocessor\n",
    "    \n",
    "    # Get model parameters from config\n",
    "    n_estimators = CONFIG['ml']['n_estimators']\n",
    "    random_state = CONFIG['ml']['random_state']\n",
    "    \n",
    "    # Define models to train - REUSING the same models as before but with config parameters\n",
    "    models = [\n",
    "        ('Random Forest', RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)),\n",
    "        ('Gradient Boosting', GradientBoostingRegressor(n_estimators=n_estimators, random_state=random_state)),\n",
    "        ('XGBoost', XGBRegressor(n_estimators=n_estimators, learning_rate=0.1, random_state=random_state))\n",
    "    ]\n",
    "    \n",
    "    # Train and evaluate all models - REUSING the existing functions\n",
    "    results = []\n",
    "    trained_pipelines = {}\n",
    "\n",
    "    for name, model in models:\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "        # REUSE build_model_pipeline\n",
    "        pipeline, _ = build_model_pipeline(model, preprocessor, selected_column_info)\n",
    "        \n",
    "        # REUSE evaluate_model_pipeline\n",
    "        trained_pipeline, metrics = evaluate_model_pipeline(\n",
    "            pipeline, X_train, X_test, y_train, y_test, name\n",
    "        )\n",
    "        \n",
    "        results.append(metrics)\n",
    "        trained_pipelines[name] = trained_pipeline\n",
    "\n",
    "    # Compare model performance with a visualization - SIMILAR to existing code\n",
    "    results_df = pd.DataFrame(results)\n",
    "    best_model = results_df.loc[results_df['test_r2'].idxmax()]\n",
    "    best_model_name = best_model['name']\n",
    "\n",
    "    print(f\"\\nBest model based on Test R² score: {best_model_name} (R² = {best_model['test_r2']:.4f})\")\n",
    "\n",
    "    # Plot R² scores\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    bars = plt.bar(results_df['name'], results_df['test_r2'], color='skyblue')\n",
    "    plt.title('Test R² Scores by Model')\n",
    "    plt.ylabel('R² Score (higher is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Highlight the best model\n",
    "    best_idx = results_df['test_r2'].idxmax()\n",
    "    bars[best_idx].set_color('darkblue')\n",
    "\n",
    "    # Plot MAE scores\n",
    "    plt.subplot(1, 2, 2)\n",
    "    bars = plt.bar(results_df['name'], results_df['test_mae'], color='lightcoral')\n",
    "    plt.title('Test MAE Scores by Model')\n",
    "    plt.ylabel('Mean Absolute Error € (lower is better)')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Highlight the best model by MAE\n",
    "    best_mae_idx = results_df['test_mae'].idxmin()\n",
    "    bars[best_mae_idx].set_color('darkred')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "    # Return best model and its pipeline\n",
    "    return best_model_name, trained_pipelines[best_model_name], results_df\n",
    "\n",
    "# Create buttons to train models with selected features\n",
    "user_select_train_button = widgets.Button(\n",
    "    description='Train with User Selected Features',\n",
    "    button_style='info',\n",
    "    tooltip='Train models with manually selected features',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "auto_select_train_button = widgets.Button(\n",
    "    description='Train with Automatically Selected Features',\n",
    "    button_style='info',\n",
    "    tooltip='Train models with automatically selected features',\n",
    "    disabled=False\n",
    ")\n",
    "\n",
    "# Functions to handle button clicks\n",
    "def on_user_select_train_clicked(b):\n",
    "    if 'user_selected_features' not in globals():\n",
    "        print(\"Please select features manually first using the feature selection widgets above\")\n",
    "        return\n",
    "    \n",
    "    global user_best_model_name, user_best_model, user_results\n",
    "    user_best_model_name, user_best_model, user_results = train_model_with_selected_features(\n",
    "        user_selected_column_info, \"user\"\n",
    "    )\n",
    "\n",
    "def on_auto_select_train_clicked(b):\n",
    "    if 'auto_selected_features' not in globals():\n",
    "        print(\"Please select features automatically first using the feature selection method above\")\n",
    "        return\n",
    "    \n",
    "    global auto_best_model_name, auto_best_model, auto_results\n",
    "    auto_best_model_name, auto_best_model, auto_results = train_model_with_selected_features(\n",
    "        auto_selected_column_info, \"automatically\"\n",
    "    )\n",
    "\n",
    "user_select_train_button.on_click(on_user_select_train_clicked)\n",
    "auto_select_train_button.on_click(on_auto_select_train_clicked)\n",
    "\n",
    "print(\"Click one of the buttons below to train models with your selected features:\")\n",
    "display(user_select_train_button)\n",
    "display(auto_select_train_button)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3105a452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models trained with different feature selection methods\n",
    "\n",
    "# Function to compare model results\n",
    "def compare_model_results():\n",
    "    # Check which models are available\n",
    "    available_models = []\n",
    "    results_list = []\n",
    "    \n",
    "    # Check original models (from earlier in the notebook)\n",
    "    if 'results_df' in globals():\n",
    "        available_models.append('Original (All Features)')\n",
    "        results_list.append(results_df)\n",
    "        \n",
    "    # Check user-selected feature models\n",
    "    if 'user_results' in globals():\n",
    "        available_models.append('User Selected Features')\n",
    "        results_list.append(user_results)\n",
    "        \n",
    "    # Check auto-selected feature models\n",
    "    if 'auto_results' in globals():\n",
    "        available_models.append('Auto Selected Features')\n",
    "        results_list.append(auto_results)\n",
    "    \n",
    "    if not available_models:\n",
    "        print(\"No models available for comparison. Please train models first.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Comparing {len(available_models)} sets of models:\")\n",
    "    print(available_models)\n",
    "    \n",
    "    # Create a comparison dataframe\n",
    "    comparison_rows = []\n",
    "    \n",
    "    # Collect results for each model set and model type\n",
    "    for i, (model_set, results) in enumerate(zip(available_models, results_list)):\n",
    "        for _, row in results.iterrows():\n",
    "            model_name = row['name']\n",
    "            comparison_rows.append({\n",
    "                'Model Set': model_set,\n",
    "                'Model Type': model_name,\n",
    "                'Test R²': row['test_r2'],\n",
    "                'Test MAE': row['test_mae'],\n",
    "                'Test RMSE': row['test_rmse']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_rows)\n",
    "    \n",
    "    # Print comparison table\n",
    "    print(\"\\nModel Performance Comparison:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Find the best overall model\n",
    "    best_model = comparison_df.loc[comparison_df['Test R²'].idxmax()]\n",
    "    print(f\"\\nBest overall model: {best_model['Model Set']} - {best_model['Model Type']}\")\n",
    "    print(f\"Test R²: {best_model['Test R²']:.4f}, Test MAE: €{best_model['Test MAE']:.2f}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Plot R² comparison\n",
    "    plt.subplot(2, 1, 1)\n",
    "    colors = ['skyblue', 'lightgreen', 'salmon']\n",
    "    \n",
    "    # Group by Model Set and Model Type\n",
    "    for i, model_set in enumerate(available_models):\n",
    "        subset = comparison_df[comparison_df['Model Set'] == model_set]\n",
    "        x_positions = [j + (i * 0.25) for j in range(len(subset))]\n",
    "        \n",
    "        plt.bar(x_positions, \n",
    "                subset['Test R²'], \n",
    "                width=0.2, \n",
    "                color=colors[i % len(colors)], \n",
    "                label=model_set)\n",
    "        \n",
    "        # Add value labels\n",
    "        for x, y in zip(x_positions, subset['Test R²']):\n",
    "            plt.text(x, y + 0.01, f'{y:.3f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.title('Test R² Score Comparison (higher is better)')\n",
    "    plt.ylabel('R² Score')\n",
    "    plt.xticks([j + 0.25 for j in range(len(subset))], subset['Model Type'], rotation=45)\n",
    "    plt.legend()\n",
    "    plt.ylim(0, comparison_df['Test R²'].max() + 0.1)\n",
    "    \n",
    "    # Plot MAE comparison\n",
    "    plt.subplot(2, 1, 2)\n",
    "    \n",
    "    # Group by Model Set and Model Type\n",
    "    for i, model_set in enumerate(available_models):\n",
    "        subset = comparison_df[comparison_df['Model Set'] == model_set]\n",
    "        x_positions = [j + (i * 0.25) for j in range(len(subset))]\n",
    "        \n",
    "        plt.bar(x_positions, \n",
    "                subset['Test MAE'], \n",
    "                width=0.2, \n",
    "                color=colors[i % len(colors)], \n",
    "                label=model_set)\n",
    "        \n",
    "        # Add value labels\n",
    "        for x, y in zip(x_positions, subset['Test MAE']):\n",
    "            plt.text(x, y + 5, f'{y:.0f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.title('Test MAE Comparison (lower is better)')\n",
    "    plt.ylabel('Mean Absolute Error (€)')\n",
    "    plt.xticks([j + 0.25 for j in range(len(subset))], subset['Model Type'], rotation=45)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Create a button to compare results\n",
    "compare_button = widgets.Button(\n",
    "    description='Compare All Models',\n",
    "    button_style='warning',\n",
    "    tooltip='Compare performance of models trained with different feature sets',\n",
    "    icon='bar-chart'\n",
    ")\n",
    "\n",
    "compare_button.on_click(lambda b: compare_model_results())\n",
    "display(compare_button)\n",
    "\n",
    "# Display help text\n",
    "display(HTML(\"\"\"<p><i>Click the button above to compare the performance of models trained \n",
    "with different feature selection methods.</i></p>\n",
    "<p><i>Note: You need to have trained at least two sets of models for comparison.</i></p>\"\"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715ef710",
   "metadata": {},
   "source": [
    "# Conclusion: Feature Selection Impact\n",
    "\n",
    "Now that we've implemented various feature selection methods, we can draw several conclusions:\n",
    "\n",
    "1. **Manual Feature Selection**:\n",
    "   - Allows domain expertise to guide the selection process\n",
    "   - Provides control over which features are included\n",
    "   - Can simplify the model and reduce noise\n",
    "\n",
    "2. **Automated Feature Selection**:\n",
    "   - Statistical methods like F-regression identify features correlated with target\n",
    "   - Recursive Feature Elimination finds the most predictive feature subset\n",
    "   - Random Forest importance provides insight into feature relevance\n",
    "\n",
    "3. **Performance Impact**:\n",
    "   - Feature selection often improves model generalization\n",
    "   - Reduces overfitting by eliminating irrelevant features\n",
    "   - Can sometimes improve R² score and reduce prediction error\n",
    "   - Makes models more interpretable\n",
    "\n",
    "4. **Best Practices**:\n",
    "   - Start with domain knowledge to select initial features\n",
    "   - Use automated methods to verify or refine selection\n",
    "   - Consider both statistical significance and practical relevance\n",
    "   - Compare models with different feature sets\n",
    "\n",
    "The comparison tool we've built allows us to clearly see the impact of feature selection on model performance and choose the optimal approach for predicting real estate prices."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
